#+STARTUP: indent

你很有可能从未仔细研究过 Linux 的音频系统. 在大多数情况下, 在装好图形界面后（或者像 ubuntu 或 openSUSE 这种装好就带桌面环境的发行版里），音频就“自然而然”地能用了。的确，对很多人来说，自带的音频设置已经够好了。

然而，隐藏在这“够好”下面的是一套颇为复杂的音频系统。如果你好奇心大发，想要了解 Linux 是怎么将你的音乐文件变成空气的震动的，抑或遇到了一点关于声音的问题，想要大概了解一下音频系统的构成以便定位问题来源，这里是一篇简短的介绍。

* 俯瞰音频系统
要让你的喇叭或者耳机动起来，我们需要经历以下的步骤：  

首先，我们将音频文件 ~解码~ 成未压缩的音频流 (waveform)。现在，为了节省空间，大部分音频文件都是压缩过的，比如无损压缩格式 =flac=, =ape= 和有损压缩格式 =mp3=, =aac=, =ogg= 等。只有经过解码这个步骤，才能将这些音频文件变成声卡可以认识的音频流。

然后，我们将这些音频流输出到一个 ~音频服务器~ (sound server) 里。比如，如果你想一边看视频一边参加一个视频会议，你就需要使用一个音频服务器把这两个音频流混合到一起以便让声卡播放。

最终，我们将混制好的音频流送到 ~声卡驱动~ 那里。由于所有音轨已经被音频服务器汇成一条，驱动仅需将音频流发送给声卡做最终的数模转换即可。

* Linux 下的具体实现
架构已经明晰，那么 Linux 下是哪些部件组成了这一架构的呢？

** ALSA: 声卡驱动
{{% btw %}}
严格来说，ALSA 是一套完整的音频处理系统（毕竟它的全名是 ~高级 Linux 音频架构~ ）。但在现在的大多数使用情况下，我们只是将它作为一个声卡驱动用。这里主要介绍实际的使用场景。
{{%/ btw %}}

ALSA 负责直接与声卡通讯。为了方便应用程序开发，ALSA 提供了一套与硬件无关的 API 以便程序能够轻松地设置输出参数{{% btw %}}（比特率，采样深度之类的）{{%/ btw %}}并发送音频流。

然而，直接使用 ALSA 有很多不方便。为了使 ALSA 能够同时播放多条音频，用户必须做一些设置 {{% btw %}}例如，是否使用声卡提供的硬件混音？还是使用软件混音器？如何调节各应用的响度？等等 {{%/ btw %}}。因此，大部分现代的桌面环境使用音频服务器解决这类问题。
